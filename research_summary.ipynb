{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import json\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Working Directory\n",
    "os.chdir('')\n",
    "papers_path = \"/papers\"\n",
    "excel_filename = \"combined_summaries.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting API Key\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define a Custom Prompt\n",
    "# This prompt instructs the model to extract the specific elements.\n",
    "prompt_template_json = \"\"\"\n",
    "You are an expert summarizer with a background in economics. Read the following research paper and create a detailed summary of it, which includes the following elements:\n",
    "1. Title, Authors, and Date of the Paper\n",
    "2. The research questions being addressed.\n",
    "3. The main findings.\n",
    "4. The data used in the study. If no data are used, state that.\n",
    "5. If there is a theoretical model, provide a summary of the model and the key results the model provides. If no model is used, state that.\n",
    "6. The empirical methodology employed, including identification assumptions.\n",
    "7. The strands of the economics literature that the paper contributes to.\n",
    "\n",
    "Your responses in steps 2-7 should be in full sentences, and providing sufficient detail to understand the paper. Provide your answer strictly in JSON format with the following keys:\"title\", \"authors\", \"date\" ,\"research_questions\", \"main_findings\", \"data_used\", \"theoretical_model\", \"empirical_methodology\", \"literature_contributions\".\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template_json, input_variables=[\"text\"])\n",
    "\n",
    "\n",
    "# Step 2: Initialize the LLM and the Summarization Chain\n",
    "# Set temperature=0 for more deterministic outputs.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "             temperature=0,\n",
    "             max_tokens=1500)  # Make sure your OpenAI API key is set in your environment\n",
    "\n",
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\",\n",
    "                             prompt=prompt)\n",
    "\n",
    "# I want to remove the parts of the document that come after the \"References\" section\n",
    "def filter_text_before_last_marker(documents, marker=\"References\"):\n",
    "    \"\"\"\n",
    "    Returns a new list of documents with content truncated at the last occurrence\n",
    "    of the marker. It includes all documents up to the document containing the \n",
    "    last occurrence of the marker. That document is truncated to only include text \n",
    "    before the last occurrence of the marker.\n",
    "    \"\"\"\n",
    "    last_index = None\n",
    "    # Find the index of the last document that contains the marker\n",
    "    for i, doc in enumerate(documents):\n",
    "        if marker in doc.page_content:\n",
    "            last_index = i\n",
    "\n",
    "    # If no document contains the marker, return the original list\n",
    "    if last_index is None:\n",
    "        return documents\n",
    "\n",
    "    # Include all documents before the one containing the last occurrence\n",
    "    filtered_docs = documents[:last_index]\n",
    "\n",
    "    # For the document with the marker, trim its content up to the last occurrence\n",
    "    doc_with_marker = documents[last_index]\n",
    "    pos = doc_with_marker.page_content.rfind(marker)\n",
    "    if pos != -1:\n",
    "        doc_with_marker.page_content = doc_with_marker.page_content[:pos]\n",
    "    filtered_docs.append(doc_with_marker)\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping w32620.pdf: already processed.\n",
      "Skipping ssrn-4154564.pdf: already processed.\n",
      "Skipping The_Perceived_Sources_of_Unexpected_Inflation.pdf: already processed.\n",
      "Skipping ssrn-4280699.pdf: already processed.\n",
      "Updated summaries saved to inflation surprises/combined_summaries.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load existing summaries if the Excel file exists\n",
    "if os.path.exists(excel_filename):\n",
    "    existing_df = pd.read_excel(excel_filename)\n",
    "    # Get a set of filenames already processed\n",
    "    processed_files = set(existing_df[\"filename\"].dropna().tolist())\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    processed_files = set()\n",
    "    \n",
    "new_summaries = []\n",
    "\n",
    "# Loop over each PDF file in the folder\n",
    "for filename in os.listdir(papers_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        if filename in processed_files:\n",
    "            print(\"Skipping {}: already processed.\".format(filename))\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(papers_path, filename)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Preprocess: remove content after \"References\" in each document chunk\n",
    "        filtered_docs = filter_text_before_last_marker(documents)\n",
    "        \n",
    "        # Run the summarization chain (it accepts a list of documents)\n",
    "        summary_output = chain.run(filtered_docs)\n",
    "        \n",
    "        # Try to parse the output as JSON. If extra text is present, extract the JSON block.\n",
    "        try:\n",
    "            summary_data = json.loads(summary_output)\n",
    "        except json.JSONDecodeError:\n",
    "            # Look for a JSON block between ```json markers\n",
    "            match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", summary_output, re.DOTALL)\n",
    "            if match:\n",
    "                summary_data = json.loads(match.group(1))\n",
    "            else:\n",
    "                # Fallback: store the raw summary output\n",
    "                summary_data = {\"raw_summary\": summary_output}\n",
    "        \n",
    "        # Add the filename to the summary data for reference\n",
    "        summary_data[\"filename\"] = filename\n",
    "        \n",
    "        # Append the summary dictionary to our list\n",
    "        new_summaries.append(summary_data)\n",
    "\n",
    "# Step 4: Update and save the Excel file\n",
    "if new_summaries:\n",
    "    new_df = pd.DataFrame(new_summaries)\n",
    "    \n",
    "    # Convert list-type values to comma-separated strings\n",
    "    for col in new_df.columns:\n",
    "        new_df[col] = new_df[col].apply(lambda x: \", \".join(map(str, x)) if isinstance(x, list) else x)\n",
    "    \n",
    "    if not existing_df.empty:\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_df\n",
    "    \n",
    "    combined_df.to_excel(excel_filename, index=False)\n",
    "    print(f\"Updated summaries saved to {excel_filename}\")\n",
    "else:\n",
    "    print(\"No new files to summarize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
